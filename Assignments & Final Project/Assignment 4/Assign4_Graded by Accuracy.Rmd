---
title: "Assignment Econ 320 Inference Exercises"
author: "Saier (Sam) Hu"
date: "April 16, 2021"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# call packages here
library(knitr)
library(tidyverse)
library(wooldridge)
library(stargazer)
library(data.table)
library(car)
```

## Wooldridge C4.11 (Modified) 


Use the data in HTV to answer this question. 

1. Estimate the regression model

$$ educ = \beta_0 + \beta_1 *motheduc + \beta_2 *fatheduc + \beta_3 *abil + \beta_4 *abil^2 + u$$
by OLS take a look at the results in the usual form but report the results using stargazer. 

2.a Test the null hypothesis that educ is linearly related to abil against the alternative that the relationship is quadratic. Show the tstat and the pvalue associated with that test. Do you reject the null hypothesis? what does that mean? EXPLAIN

- We fail to reject the null hypothesis. Since the result of the regression suggests that we reject the hypothesis that parameter for "abil" is 0, we know that the coefficient for education is not 0. Thus, education is linearly related to ability

2.b Test the null hypothesis that the effect of mothereduc is bigger 0.3. 

```{r,warning = 'FALSE'}
# to include the term ability squared you can create a separate variable or even aesier use the I(function) in the lm command to add the term 

# abil2<-htv$abil^2 this will create the variable separate, but better to use I(abil^2)
data(htv,package = "wooldridge")
m1 <- lm(educ ~ motheduc + fatheduc + abil + I(abil^2),htv)
stargazer(m1,type = "text", covariate.labels = c("motheduc","fatheduc","abil","I(abil2)","Constant"))
```



```{r}
# Reproduce t statistic
# When parenthesis are added the object is printed
regtable <- summary(m1)$coefficients
bhat <- regtable[5,1]
se   <- regtable[5,2]
tstat <- (bhat - 0) / se
print(paste("T-test for linear vs quadratic relation on ability"))
print(tstat)

# Reproduce p value
print(paste("pvalue = ", round((pval<-2*pt(-abs(tstat),1225)), 5)))

print("T-test for mother educ > 0.3")
# Reproduce t statistic
bhat1 <- regtable[2,1]
se1   <- regtable[2,2]
tstat1 <- (bhat1 - 0.3) / se1
(tstat1)

# When parenthesis are added the object is printed
 # Is the same as doing bhat / se but it allows you to see where to add the value if different than zero 
# Reproduce p value
print(paste("pvalue = ", round((pval<-pt(abs(tstat1),1225)), 6)))

```

<br>

3.  Using the equation in part (2), test $H_0: \beta_1=\beta_2$ against a two-sided alternative. What is the p-value of the test? 

- The p-value is 0.053, and we fail to reject the null hypothesis.

Remember this requires for creating a new regression with a $\theta_1=\beta_1-\beta_2$ as shown in your book and then test for $H_0: \theta_1=0$

 Let's change the regression to create $\theta_1=\beta_1-\beta_2$ 

Add and subrstact $\beta_2 motheduc$ and create a variable $parentedu=motheduc+fatheduc$ see below: 

$$ educ = \beta_0 + \beta_1 motheduc - \beta_2 motheduc + \beta_2 motheduc+ \beta_2 fatheduc + \beta_3 abil + \beta_4 abil^2 + u$$

$$ educ = \beta_0 + (\beta_1 - \beta_2)   motheduc + \beta_2  (motheduc+fatheduc) + \beta_3 abil + \beta_4 abil^2 + u$$
$$ educ = \beta_0 + \theta_1   motheduc + \beta_2  (parentedu) + \beta_3 abil + \beta_4 abil^2 + u$$

By testing the null hypothesis that $H_0:\theta_1=0$ with $alpha=0.05$ we are testing $H_0: \beta_1=\beta_2$ So we just run the regression that has $\theta_1$ as a regressor and look at the t-test for $\theta_1$

```{r}
#critical Values for alpha=5% and 1% for 1225 degrees of freedom 
print("critical values for alpha 5% and 1% 2 tails")
alpha_2 <- c(0.025, 0.005)
print(qnorm(alpha_2))

# create parenteduc
parenteduc <- htv$motheduc + htv$fatheduc

# regression with theta1
m2 <- lm(educ ~ motheduc + parenteduc+abil+I(abil^2),data=htv)
stargazer(m2,type = "text", covariate.labels = c("motheduc","parenteduc","abil","I(abil2)","Constant"))

```


***
**Use in-line code and your interpretation for this paragraph** the value of $\theta_1$ is equal to `r round(summary(m2)$coefficients[2,1],3)` with a t-stat of `r round(summary(m2)$coefficients[2,3],3)` and a p-value of `r round(summary(m2)$coefficients[2,4],3)` this means that we fail to reject the null hypothesis that  $H_0:\theta_1=0$ which means that $\beta_1$ = $\beta_2$ therefore the level of education of mother's and father's have equal magnitude. 

***
<br>

4. 	Add the two college tuition variables to the regression from part (2) and determine whether they are jointly statistically significant. 
First do the F-test step-by-step

```{r Ftest}
# CV for alpha=1% using the F distribution with 1223 degrees of freedom d.f. :
qf(1-0.01, 2, 1223)

## F test step by step
# Unrestricted OLS regression:
res.ur <- lm(educ ~ motheduc + fatheduc + abil + I(abil^2) + tuit17 + tuit18,data = htv)

# Restricted OLS regression:
res.r <- lm(educ ~ motheduc + fatheduc + abil + I(abil^2),data = htv)

# R2:
r2.ur <- summary(res.ur)$r.squared 	# R squared unrestricted
r2.r <- summary(res.r)$r.squared # R squared restricted 
print(paste("$R^2$ unrestricted=", r2.ur))
print(paste("$R^2$ restricted=", r2.r))

# F statistic:
F <- (r2.ur-r2.r) / (1-r2.ur) * res.ur$df/2
print(paste("F-stat=", F))

# p value = 1-cdf of the appropriate F distribution:
print(paste("p-value=", round(1-pf(F, 2, res.ur$df),3)))
```
<br>
***
Then use any of the other methods
 <br>
```{r}
# F test 
myH0 <- c("tuit17","tuit18")

linearHypothesis(res.ur,myH0)

# anova(res, res.ur)

```


<br> 
This shows that in this case we **fail to reject the null hypothesis**, that the coefficients are jointly zero. 

***
5.  Use function `confint()` to find the confidence intervals of all the parameters in the unrestricted model from (4) What do you conclude? EXPLAIN this results in the light of the significance of your coefficients

- The fitted values for the parameters of explanatory in unrestricted model suggests that most of the fitted values fall into the 95% of the confidence level.
- More importantly, this shows that for the parameters of "tuit17"and "tuit18", 0 is in the confidence intervals. Thus, we fail to reject the null hypothesis with 95% confidence.

```{r}
confint(res.ur,level=0.95)
```



***
<style>
div.gray { background-color:#dbdbdb; border-radius: 5px; padding: 20px;}
</style>
<div class = "gray">

**Packages used in this document**



</div>
<br>
<br>







